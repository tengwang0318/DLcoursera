### key concepts on deep neural networks

1. Vectorization allows you to compute forward propagation in an *L*-layer neural network without an explicit for-loop (or any other explicit iterative loop) over the layers l=1, 2, â€¦,L. True/False?

   ### answer:
   False, Forward propagation propagates the input through the layers, although for shallow networks we may just write all the lines $a^{[2]}=g^{[2]}(z^{[2]})$ $z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}...$ in a deeper network, we cannot avoid a for loop iterating over the layers: $a^{[l]}=g^{[l]}(z^{[l]}),z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}$

   